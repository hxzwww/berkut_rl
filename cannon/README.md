Обучение - `training.ipynb`

Классы актора, критика, среды - `cannon.py`

Дамп сети - `actor.pt`

Можно пострелять - `shoot.ipynb`

#### Обучение:

Задача решена при помощи алгоритма [DDPG](https://spinningup.openai.com/en/latest/algorithms/ddpg.html).

Так как DDPG достаточно нестабильный алгоритм, а мои вычислительные возможности сильно ограничены, я установил некоторые ограничения на стрельбу пушки, а именно дистанция не больше 1000 м, угол от 30 до 60 градусов. Эти ограничения нужны для относитльно небольших получаемых значений скорости (так как с такими ограничениями скорость не может быть больше 100 м/c) и возможности использовать не очень большие сетки, таким образом добиваясь более быстрого и стабильного обучения. Понятно, что ограничения можно ослабить и алгоритм все равно будет работать, но на сетях побольше и обучении подольше.

За `reward` я брал минус модуль разности дистанции попадания и дистанции до цели.
Так как после того как мы выбираем действие, мы переходим в состояние аналогичное исходному (угол тот же, дистанция другая), наша игра как бы является одношаговой, и при пересчете $Q$-значений мы берем $Q_1(s, a) = reward$, т.е. следущие $Q$ мы полагаем нулевыми.
Таким образом, критик просто пытался приближать функцию реворда среды, и таргет сеть критика нам не нужна.

#### Результат:

В существующих ограничениях наша пушка стреляет очень даже точно, среднее отклонение - 1.7 метров, медианное - 0.8, да и в целом у нас 95% выстрелов попадают в радиус 7 метров.
