Обучение - `training.ipynb`

Дамп агента - `agent.pickle`

Класс агента - `ttt_agent.py`

Класс среды - `ttt_env.py`

Можно сыграть - `play.ipynb` (необходим только `numpy`)

### Метрика качества игры агента:
Я проверял как играет агент на всех развитиях партии, то есть я рекурсивно перебирал всевозможные ответы на ходы агента и считал долю побед/поражений/ничей.
Это корректно, тк наш агент детерминированный.


### Обучение:
Агент играет сам с собой, то есть реализован мультиагентный подход, но так как нам нужны $Q_{t+1}$ для пересчета $Q_t$, а состояния противников чередуются и не перескаются, можно объединить их в одного игрока

Обучение происходит при помощи немного измененного алгоритма `Q-learnging`. 

##### Общий пайплайн:

- Агент играет партию до завершения, при этом каждое (состояние, ход) запоминается
- Если победа: победный ход получает `reward = 1`, предпоследний ход получает `reward = -1e10` (так как он очевидно проигрывает и нужно его сразу исключить)
- Если ничья: последний и предпоследний ходы получают `reward = 0.1` (тк ничья это в конце концов самый частый результат и нужно это немного поощрять)
- Дальше все ходы от пред-предпоследнего получают `reward`, уменьшающийся от конца к началу (в случае если игра закончилась победой, то ходы проигравшего получаются `-reward`). Таким образом агент получает значительно больше информации, и немного корректирует стратегию на всех этапах игры.
- В обновлении $Q$-значений добавлено небольшое изменение, а именно $Q(s, a) = Q(s, a) + lr * (reward - df * max_a{Q(next \textunderscore s, a) - Q(s, a)})$, то есть $Q(next \textunderscore s, a)$ добавляется со знаком минус. Это обусловлено тем, что $Q$ от следующих состояний относится к оппоненту, поэтому следует исключать $Q(s, a)$ приводящие к большим $Q(next \textunderscore s, a)$

Все эти изменения относительно обычного `Q-learnging` только улучшали качество игры агента

В конце концов обучение агента привело к тому, что он почти идельно играл за `X` и проигрывал около 15-17% за `O`. Путем взаимодействия с агентом, было выяснено что он проигрывает против +- одинаковых начальных ходов, поэтому был сделан вывод что после обучения агент становится хорошим игроком лишь в части всевозвожных развитий партии. Очевидно, что от обучения не зависит, где агент будет играть хорошо, а где плохо, поэтому было принято решение объединение знаний нескольких обученных агентов (получаем нового агента просто складывая их распределения). Это корректно тк каждый из них хорош в своей области и хороших ходов значительно больше, то есть в конкретной ситуации меньшее количество агентов хотят принять плохое действие, поэтому суммарная $Q$ хорошего хода будет больше

В итоге мы получили нового агента, который является неявным ансамблем более слабых агентов. Он уже не проигрывал ни одной партии и путем добавления в него все новых агентов мы максимизировали процент побед и достигли оптимальной стратегии (можно в этом убедиться, попробовав выиграть его). 

### Результат:
Я получил детерминированного агента, который никогда не проигрывает и не было выявлено ситуаций, когда он упускает возможность победить (проверка этого была посредством запуска игры агента с человеком с доски, на которой уже сделано несколько первых ходов, таких комбинаций не так много)
